



| [de] dictionary: 8848 types
| [en] dictionary: 6632 types
| loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.de
| loaded 7283 examples from: data-bin/iwslt14.tokenized.de-en/valid.de-en.en
| data-bin/iwslt14.tokenized.de-en valid de-en 7283 examples
FirstTokenCrossEntropyCriterion()
| loaded checkpoint checkpoints/checkpoint_best.pt (epoch 37 @ 0 updates)
| loading train data for epoch 37
| loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.de
| loaded 160239 examples from: data-bin/iwslt14.tokenized.de-en/train.de-en.en
| data-bin/iwslt14.tokenized.de-en train de-en 160239 examples
Training Loss: 1.4127618074417114
revolution hlen glob@@ tre@@ schwer@@ weiterhin nern rb profi@@ at
Training Loss: 1.4209295511245728
kam hlen glob@@ tre@@ schwer@@ weiterhin nern rb profi@@ at
Training Loss: 1.4283020496368408
kam hlen glob@@ tre@@ wollte weiterhin nern rb profi@@ at
Training Loss: 1.4402949810028076
kam hlen glob@@ ution wollte weiterhin nern rb profi@@ at
Training Loss: 1.446022629737854
kam hlen glob@@ ution wollte weiterhin van rb profi@@ at
Training Loss: 1.4533940553665161
kam hlen nannte ution wollte weiterhin van rb profi@@ at
Training Loss: 1.4616271257400513
kam hlen nannte ution wollte began van rb profi@@ at
Training Loss: 1.4842268228530884
kam täusch@@ nannte ution wollte began van rb profi@@ at
Training Loss: 1.4955838918685913
aid täusch@@ nannte ution wollte began van rb profi@@ at
Training Loss: 1.5017125606536865
aid täusch@@ nannte ution wollte began van rb lernen at
Training Loss: 1.5025115013122559
aid täusch@@ nannte ution wollte began van rb wovon at
Training Loss: 1.511820912361145
aid täusch@@ nannte sie wollte began van rb wovon at
Training Loss: 1.5139509439468384
aid täusch@@ nannte sie wollte began van rb wovon an
Training Loss: 1.522634506225586
aid täusch@@ nannte sie wollte began van rb euch an
Training Loss: 1.5302371978759766
tigung täusch@@ nannte sie wollte began van rb euch an
Training Loss: 1.5316158533096313
tigung täusch@@ nannte sie wollte began van produzieren euch an
Training Loss: 1.5391818284988403
tigung täusch@@ nannte sie wollte began van konnte euch an
Training Loss: 1.5393184423446655
tigung täusch@@ nannte sie wollte began van konnte euch falt
Training Loss: 1.5401091575622559
tigung täusch@@ nannte sie wollte began van konnte euch ases
Training Loss: 1.5446338653564453
tigung täusch@@ nannte sie wollte began van konnte euch wollte
Training Loss: 1.5479178428649902
ck täusch@@ nannte sie wollte began van konnte euch wollte
Training Loss: 1.5520169734954834
ck täusch@@ ption sie wollte began van konnte euch wollte
Training Loss: 1.5536284446716309
ck täusch@@ ption sie wollte began seem@@ konnte euch wollte
Training Loss: 1.5564912557601929
ck täusch@@ ption sie wollte mich@@ seem@@ konnte euch wollte
Training Loss: 1.5568387508392334
ck täusch@@ ption sie wollte mich@@ energ@@ konnte euch wollte
Training Loss: 1.5571787357330322
ck täusch@@ ption sie wollte mich@@ wäl@@ konnte euch wollte
Training Loss: 1.5601872205734253
ck täusch@@ ption sie wollte texas wäl@@ konnte euch wollte
Training Loss: 1.5657525062561035
ck täusch@@ cer sie wollte texas wäl@@ konnte euch wollte
Training Loss: 1.5715566873550415
ck täusch@@ cer sie sprach texas wäl@@ konnte euch wollte
Training Loss: 1.574815273284912
ck täusch@@ 2004 sie sprach texas wäl@@ konnte euch wollte
Training Loss: 1.5751653909683228
ck täusch@@ 2004 sie lin texas wäl@@ konnte euch wollte
Training Loss: 1.5811032056808472
ck täusch@@ 2004 sie wusste texas wäl@@ konnte euch wollte
^[[A^[[A^[[A^_Training Loss: 1.5840330123901367
ck täusch@@ 2004 sie wusste 2004 wäl@@ konnte euch wollte
Training Loss: 1.5870388746261597
ck täusch@@ 2004 sie wusste 2004 ersetzen konnte euch wollte
Training Loss: 1.5881623029708862
ck täusch@@ 2004 sie wusste 2004 emergen@@ konnte euch wollte
Validation Loss Using First Token Cross Entropy:  tensor(1.5209)
Validation Loss Using First Token Cross Entropy:  tensor(1.5209)
^[[A^[[A^[[A^[[A^[[A^[[A^[[AValidation Loss Using First Token Cross Entropy:  tensor(1.5209)
Validation Loss Using First Token Cross Entropy:  tensor(1.5209)
Validation Loss Using First Token Cross Entropy:  tensor(1.5209)
Training Loss: 1.762603759765625
ck täusch@@ 2004 sie also 2004 emergen@@ konnte euch wollte
Training Loss: 1.808296799659729
later täusch@@ 2004 sie also 2004 emergen@@ konnte euch wollte
Training Loss: 1.8515830039978027
later zukunft 2004 sie also 2004 emergen@@ konnte euch wollte
Training Loss: 1.8795592784881592
later zukunft 2004 sie also 2004 emergen@@ konnte euch smith
Training Loss: 1.9064931869506836
later zukunft 2004 sie also tionen emergen@@ konnte euch smith
Training Loss: 1.950307846069336
later zukunft 2004 sie also tionen emergen@@ konnte electr@@ smith
Training Loss: 1.9707907438278198
later zukunft 2004 sie also gore emergen@@ konnte electr@@ smith
Training Loss: 2.0086846351623535
later zukunft 2004 sie also gore ausdruck konnte electr@@ smith
Training Loss: 2.0286898612976074
later zukunft 2004 sie also gore ausdruck konnte welchem smith
Training Loss: 2.0427961349487305
later zukunft 2004 sie also gore ersten konnte welchem smith
Training Loss: 2.0641725063323975
vi@@ zukunft 2004 sie also gore ersten konnte welchem smith
Training Loss: 2.0654335021972656
vi@@ zukunft 2004 sie also gore licherweise konnte welchem smith
Training Loss: 2.0798542499542236
vi@@ zukunft 2004 sie also gore licherweise konnte welchem cin
Training Loss: 2.107807159423828
vi@@ zukunft 2004 sie also gore licherweise versuchten welchem cin
Training Loss: 2.1094062328338623
vi@@ zukunft 2004 sie also gore licherweise versuchten welchem galaxien
Training Loss: 2.1244595050811768
vi@@ zukunft 2004 sie also gore alien versuchten welchem galaxien
Training Loss: 2.12605357170105
vi@@ zukunft 2004 sie also gore erstes versuchten welchem galaxien
Training Loss: 2.1347291469573975
vi@@ zukunft 2004 sie also gore daran versuchten welchem galaxien
Validation Loss Using First Token Cross Entropy:  tensor(1.9786)


